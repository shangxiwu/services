{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將句子轉換成token\n",
    "## 將垃圾郵件資料集每個句子轉成token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 \n",
      "text_data_target: ham \n",
      "text_data_train: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "i = 1 \n",
      "text_data_target: ham \n",
      "text_data_train: Ok lar... Joking wif u oni...\n",
      "\n",
      "i = 2 \n",
      "text_data_target: spam \n",
      "text_data_train: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "i = 3 \n",
      "text_data_target: ham \n",
      "text_data_train: U dun say so early hor... U c already then say...\n",
      "\n",
      "i = 4 \n",
      "text_data_target: ham \n",
      "text_data_train: Nah I don't think he goes to usf, he lives around here though\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Set RNN parameters\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "max_sequence_length = 25\n",
    "rnn_size = 10\n",
    "embedding_size = 50\n",
    "min_word_frequency = 10\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Download or open data\n",
    "data_dir = 'temp'\n",
    "data_file = 'text_data.txt'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "\n",
    "    # Save data to text file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as file_conn:\n",
    "        for text in text_data:\n",
    "            file_conn.write(\"{}\\n\".format(text))\n",
    "else:\n",
    "    # Open data from text file\n",
    "    text_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        for row in file_conn:\n",
    "            text_data.append(row)\n",
    "    text_data = text_data[:-1]\n",
    "\n",
    "text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "\n",
    "for i in range(5):\n",
    "    print('i = {} \\ntext_data_target: {} \\ntext_data_train: {}'.format(i, text_data_target[i], text_data_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "ok lar joking wif u oni\n",
      "free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry questionstd txt ratetcs apply overs\n",
      "u dun say so early hor u c already then say\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "freemsg hey there darling its been weeks now and no word back id like some fun you up for it still tb ok xxx std chgs to send to rcv\n",
      "even my brother is not like to speak with me they treat me like aids patent\n",
      "as per your request melle melle oru minnaminunginte nurungu vettam has been set as your callertune for all callers press to copy your friends callertune\n",
      "winner as a valued network customer you have been selected to receivea prize reward to claim call claim code kl valid hours only\n",
      "had your mobile months or more u r entitled to update to the latest colour mobiles with camera for free call the mobile update co free on\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-dbaefe3edd4b>:16: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "=================\n",
      "[ 44 455   0 809 703 667  62   9   0  87 120 366   0 152   0   0  66  56\n",
      "   0 136   0   0   0   0   0]\n",
      "[ 47 315   0 458   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[ 46 465   9   4 773 882   1 178   0   0 622   0 236 258  69   0   1   1\n",
      " 318 465   0  77   0 368   0]\n",
      "[  6 230 140  23 357   0   6 155 142  58 140   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[  0   2  42  97  70 466   1 928  70   0 202 110 473   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return(text_string)\n",
    "\n",
    "# Clean texts\n",
    "text_data_train = [clean_text(x) for x in text_data_train]\n",
    "\n",
    "## write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
