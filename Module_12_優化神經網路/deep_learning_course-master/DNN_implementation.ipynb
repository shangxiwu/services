{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implement Overview  \n",
    "\n",
    "Recall that we have three step to build and train our neural network.\n",
    "1. Build model\n",
    "2. Define cost\n",
    "3. Optimization \n",
    "\n",
    "This concept is the same when coding in tensorflow. Usually, you code should look like this.\n",
    "\n",
    "![Alt text](./images/dnn_implement/tf_schema.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "MNIST is a computer vision dataset. It consists of black and white images from zero to nine. Each image is 28 * 28 and have been flatten to 784 dimension vector. Also, it includes labels for each image, telling us which digit it is.\n",
    "\n",
    "![Alt text](./images/dnn_implement/Selection_017.png)\n",
    "![Alt text](./images/dnn_implement/Selection_018.png)\n",
    "\n",
    "\n",
    "The MNIST data is split into three parts: \n",
    "1. 55000 training data (mnist.train) with a shape of [55000, 784]\n",
    "2. 10000 test data (mnist.test) with a shape of [10000, 784]\n",
    "3. 5000 validation data (mnist.validation) with a shape of [5000, 784]\n",
    "\n",
    "you can access:  \n",
    "training image as `mnist.train.images` (see below picture)  \n",
    "training label as `mnist.train.labels` (see below picture)  \n",
    "test image as `mnist.test.images`   \n",
    "test label as `mnist.test.labels`   \n",
    "\n",
    "Note that label is encoded as \"one-hot vectors\", which mean if the target image is 2, the label should be [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "![Alt text](./images/dnn_implement/Selection_021.png)\n",
    "![Alt text](./images/dnn_implement/Selection_020.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.3025853633880615\n",
      "step 50: cross_entropy is 0.34718087315559387\n",
      "step 100: cross_entropy is 0.35041946172714233\n",
      "step 150: cross_entropy is 0.3277951180934906\n",
      "step 200: cross_entropy is 0.3836279809474945\n",
      "step 250: cross_entropy is 0.5161219239234924\n",
      "step 300: cross_entropy is 0.23777730762958527\n",
      "step 350: cross_entropy is 0.37666282057762146\n",
      "step 400: cross_entropy is 0.33953574299812317\n",
      "step 450: cross_entropy is 0.3562777638435364\n",
      "step 500: cross_entropy is 0.2710689604282379\n",
      "step 550: cross_entropy is 0.3360309898853302\n",
      "step 600: cross_entropy is 0.23308411240577698\n",
      "step 650: cross_entropy is 0.30350497364997864\n",
      "step 700: cross_entropy is 0.2830130457878113\n",
      "step 750: cross_entropy is 0.34542033076286316\n",
      "step 800: cross_entropy is 0.34182003140449524\n",
      "step 850: cross_entropy is 0.2357882708311081\n",
      "step 900: cross_entropy is 0.20600156486034393\n",
      "step 950: cross_entropy is 0.29457005858421326\n",
      "Testing...... accuracy is 0.9186999797821045\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Load mnist dataset\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# Define image input 784 = 28 * 28. Note that DNN input is a vector\n",
    "# [None, 784] mean that there are a batch of data and each of them is 784 dimension vector\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define label. There are totally 10 class (0-9)\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y_predict = tf.matmul(x, W) + b\n",
    "\n",
    "\n",
    "# Define cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Calculate accuracy \n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        \n",
    "        train_step_, cross_entropy_ = sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            # print cross_entropy every 50 steps\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    # Load test data to validate the model  \n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how DNN with three hidden layers to improve the work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.329409122467041\n",
      "step 50: cross_entropy is 1.0036280155181885\n",
      "step 100: cross_entropy is 1.1816593408584595\n",
      "step 150: cross_entropy is 0.8087384104728699\n",
      "step 200: cross_entropy is 0.9154343605041504\n",
      "step 250: cross_entropy is 0.67885422706604\n",
      "step 300: cross_entropy is 0.6047723293304443\n",
      "step 350: cross_entropy is 0.20516090095043182\n",
      "step 400: cross_entropy is 0.11308196187019348\n",
      "step 450: cross_entropy is 0.10255002975463867\n",
      "step 500: cross_entropy is 0.14689059555530548\n",
      "step 550: cross_entropy is 0.09609723091125488\n",
      "step 600: cross_entropy is 0.13839812576770782\n",
      "step 650: cross_entropy is 0.24563270807266235\n",
      "step 700: cross_entropy is 0.15865127742290497\n",
      "step 750: cross_entropy is 0.12630750238895416\n",
      "step 800: cross_entropy is 0.17303167283535004\n",
      "step 850: cross_entropy is 0.2008529156446457\n",
      "step 900: cross_entropy is 0.03745562955737114\n",
      "step 950: cross_entropy is 0.11438455432653427\n",
      "Testing...... accuracy is 0.9672999978065491\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "INPUT_NODE =784\n",
    "\n",
    "LAYER1_NODE = 128\n",
    "LAYER2_NODE = 64\n",
    "LAYER3_NODE = 10\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([LAYER1_NODE], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, LAYER2_NODE], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([LAYER2_NODE], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE, LAYER3_NODE], stddev=0.1))\n",
    "b3 = tf.Variable(tf.truncated_normal([LAYER3_NODE], stddev=0.1))\n",
    "\n",
    "layer_1 = tf.matmul(x, W1) + b1\n",
    "out1 = tf.nn.relu(layer_1)\n",
    "layer_2 = tf.matmul(out1, W2) + b2\n",
    "out2 = tf.nn.relu(layer_2)\n",
    "layer_3 = tf.matmul(out2, W3) + b3\n",
    "out3 = tf.nn.relu(layer_3)\n",
    "\n",
    "y_predict = out3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        \n",
    "        train_step_, cross_entropy_ =sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about if we change learning rate very big?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.314532995223999\n",
      "step 50: cross_entropy is 2.3025853633880615\n",
      "step 100: cross_entropy is 2.3025853633880615\n",
      "step 150: cross_entropy is 2.3025853633880615\n",
      "step 200: cross_entropy is 2.3025853633880615\n",
      "step 250: cross_entropy is 2.3025853633880615\n",
      "step 300: cross_entropy is 2.3025853633880615\n",
      "step 350: cross_entropy is 2.3025853633880615\n",
      "step 400: cross_entropy is 2.3025853633880615\n",
      "step 450: cross_entropy is 2.3025853633880615\n",
      "step 500: cross_entropy is 2.3025853633880615\n",
      "step 550: cross_entropy is 2.3025853633880615\n",
      "step 600: cross_entropy is 2.3025853633880615\n",
      "step 650: cross_entropy is 2.3025853633880615\n",
      "step 700: cross_entropy is 2.3025853633880615\n",
      "step 750: cross_entropy is 2.3025853633880615\n",
      "step 800: cross_entropy is 2.3025853633880615\n",
      "step 850: cross_entropy is 2.3025853633880615\n",
      "step 900: cross_entropy is 2.3025853633880615\n",
      "step 950: cross_entropy is 2.3025853633880615\n",
      "Testing...... accuracy is 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "INPUT_NODE =784\n",
    "\n",
    "LAYER1_NODE = 128\n",
    "LAYER2_NODE = 64\n",
    "LAYER3_NODE = 10\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([LAYER1_NODE], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, LAYER2_NODE], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([LAYER2_NODE], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE, LAYER3_NODE], stddev=0.1))\n",
    "b3 = tf.Variable(tf.truncated_normal([LAYER3_NODE], stddev=0.1))\n",
    "\n",
    "layer_1 = tf.matmul(x, W1) + b1\n",
    "out1 = tf.nn.relu(layer_1)\n",
    "layer_2 = tf.matmul(out1, W2) + b2\n",
    "out2 = tf.nn.relu(layer_2)\n",
    "layer_3 = tf.matmul(out2, W3) + b3\n",
    "out3 = tf.nn.relu(layer_3)\n",
    "\n",
    "y_predict = out3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.GradientDescentOptimizer(2).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        \n",
    "        train_step_, cross_entropy_ =sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How about if we initialize all of our variables zeros? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.3025853633880615\n",
      "step 50: cross_entropy is 2.3025853633880615\n",
      "step 100: cross_entropy is 2.3025853633880615\n",
      "step 150: cross_entropy is 2.3025853633880615\n",
      "step 200: cross_entropy is 2.3025853633880615\n",
      "step 250: cross_entropy is 2.3025853633880615\n",
      "step 300: cross_entropy is 2.3025853633880615\n",
      "step 350: cross_entropy is 2.3025853633880615\n",
      "step 400: cross_entropy is 2.3025853633880615\n",
      "step 450: cross_entropy is 2.3025853633880615\n",
      "step 500: cross_entropy is 2.3025853633880615\n",
      "step 550: cross_entropy is 2.3025853633880615\n",
      "step 600: cross_entropy is 2.3025853633880615\n",
      "step 650: cross_entropy is 2.3025853633880615\n",
      "step 700: cross_entropy is 2.3025853633880615\n",
      "step 750: cross_entropy is 2.3025853633880615\n",
      "step 800: cross_entropy is 2.3025853633880615\n",
      "step 850: cross_entropy is 2.3025853633880615\n",
      "step 900: cross_entropy is 2.3025853633880615\n",
      "step 950: cross_entropy is 2.3025853633880615\n",
      "Testing...... accuracy is 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "INPUT_NODE =784\n",
    "\n",
    "LAYER1_NODE = 128\n",
    "LAYER2_NODE = 64\n",
    "LAYER3_NODE = 10\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.zeros([INPUT_NODE, LAYER1_NODE]))\n",
    "b1 = tf.Variable(tf.zeros([LAYER1_NODE]))\n",
    "W2 = tf.Variable(tf.zeros([LAYER1_NODE, LAYER2_NODE]))\n",
    "b2 = tf.Variable(tf.zeros([LAYER2_NODE]))\n",
    "W3 = tf.Variable(tf.zeros([LAYER2_NODE, LAYER3_NODE]))\n",
    "b3 = tf.Variable(tf.zeros([LAYER3_NODE]))\n",
    "\n",
    "layer_1 = tf.matmul(x, W1) + b1\n",
    "out1 = tf.nn.relu(layer_1)\n",
    "layer_2 = tf.matmul(out1, W2) + b2\n",
    "out2 = tf.nn.relu(layer_2)\n",
    "layer_3 = tf.matmul(out2, W3) + b3\n",
    "out3 = tf.nn.relu(layer_3)\n",
    "\n",
    "y_predict = out3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        \n",
    "        train_step_, cross_entropy_ =sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about change batch size to one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.4037392139434814\n",
      "step 50: cross_entropy is 2.3025851249694824\n",
      "step 100: cross_entropy is 2.3025851249694824\n",
      "step 150: cross_entropy is 2.3025851249694824\n",
      "step 200: cross_entropy is 2.3025851249694824\n",
      "step 250: cross_entropy is 2.3025851249694824\n",
      "step 300: cross_entropy is 2.3025851249694824\n",
      "step 350: cross_entropy is 2.3025851249694824\n",
      "step 400: cross_entropy is 2.3025851249694824\n",
      "step 450: cross_entropy is 2.3025851249694824\n",
      "step 500: cross_entropy is 2.3025851249694824\n",
      "step 550: cross_entropy is 2.3025851249694824\n",
      "step 600: cross_entropy is 2.3025851249694824\n",
      "step 650: cross_entropy is 2.3025851249694824\n",
      "step 700: cross_entropy is 2.3025851249694824\n",
      "step 750: cross_entropy is 2.3025851249694824\n",
      "step 800: cross_entropy is 2.3025851249694824\n",
      "step 850: cross_entropy is 2.3025851249694824\n",
      "step 900: cross_entropy is 2.3025851249694824\n",
      "step 950: cross_entropy is 2.3025851249694824\n",
      "Testing...... accuracy is 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "INPUT_NODE =784\n",
    "\n",
    "LAYER1_NODE = 128\n",
    "LAYER2_NODE = 64\n",
    "LAYER3_NODE = 10\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([LAYER1_NODE], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, LAYER2_NODE], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([LAYER2_NODE], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE, LAYER3_NODE], stddev=0.1))\n",
    "b3 = tf.Variable(tf.truncated_normal([LAYER3_NODE], stddev=0.1))\n",
    "\n",
    "layer_1 = tf.matmul(x, W1) + b1\n",
    "out1 = tf.nn.relu(layer_1)\n",
    "layer_2 = tf.matmul(out1, W2) + b2\n",
    "out2 = tf.nn.relu(layer_2)\n",
    "layer_3 = tf.matmul(out2, W3) + b3\n",
    "out3 = tf.nn.relu(layer_3)\n",
    "\n",
    "y_predict = out3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(1)\n",
    "\n",
    "        train_step_, cross_entropy_ =sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about change different optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0: cross_entropy is 2.3241727352142334\n",
      "step 50: cross_entropy is 0.6834895610809326\n",
      "step 100: cross_entropy is 0.6634771823883057\n",
      "step 150: cross_entropy is 0.36657410860061646\n",
      "step 200: cross_entropy is 0.4801430106163025\n",
      "step 250: cross_entropy is 0.35082969069480896\n",
      "step 300: cross_entropy is 0.361601859331131\n",
      "step 350: cross_entropy is 0.4063527584075928\n",
      "step 400: cross_entropy is 0.3643345534801483\n",
      "step 450: cross_entropy is 0.32750022411346436\n",
      "step 500: cross_entropy is 0.34682929515838623\n",
      "step 550: cross_entropy is 0.3291756510734558\n",
      "step 600: cross_entropy is 0.44918328523635864\n",
      "step 650: cross_entropy is 0.2542789876461029\n",
      "step 700: cross_entropy is 0.27747201919555664\n",
      "step 750: cross_entropy is 0.24614015221595764\n",
      "step 800: cross_entropy is 0.1614810973405838\n",
      "step 850: cross_entropy is 0.3211557865142822\n",
      "step 900: cross_entropy is 0.12491820007562637\n",
      "step 950: cross_entropy is 0.23574095964431763\n",
      "Testing...... accuracy is 0.8806999921798706\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "INPUT_NODE =784\n",
    "\n",
    "LAYER1_NODE = 128\n",
    "LAYER2_NODE = 64\n",
    "LAYER3_NODE = 10\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "b1 = tf.Variable(tf.truncated_normal([LAYER1_NODE], stddev=0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, LAYER2_NODE], stddev=0.1))\n",
    "b2 = tf.Variable(tf.truncated_normal([LAYER2_NODE], stddev=0.1))\n",
    "W3 = tf.Variable(tf.truncated_normal([LAYER2_NODE, LAYER3_NODE], stddev=0.1))\n",
    "b3 = tf.Variable(tf.truncated_normal([LAYER3_NODE], stddev=0.1))\n",
    "\n",
    "layer_1 = tf.matmul(x, W1) + b1\n",
    "out1 = tf.nn.relu(layer_1)\n",
    "layer_2 = tf.matmul(out1, W2) + b2\n",
    "out2 = tf.nn.relu(layer_2)\n",
    "layer_3 = tf.matmul(out2, W3) + b3\n",
    "out3 = tf.nn.relu(layer_3)\n",
    "\n",
    "y_predict = out3\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predict))\n",
    "train_step = tf.train.AdamOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "\n",
    "        train_step_, cross_entropy_ =sess.run([train_step, cross_entropy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if step % 50 == 0:\n",
    "            print(\"step {}: cross_entropy is {}\".format(step, cross_entropy_))\n",
    "    accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "    print('Testing...... accuracy is {}'.format(accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also define your model in a function to make your code more elegent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/envs/py3-gpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 56.33713734713476\n",
      "Epoch 2, cost= 15.839478158083825\n",
      "Epoch 3, cost= 9.99515647151253\n",
      "Epoch 4, cost= 7.184229089075868\n",
      "Epoch 5, cost= 5.464726668433704\n",
      "Epoch 6, cost= 4.318168989528308\n",
      "Epoch 7, cost= 3.480042829892852\n",
      "Epoch 8, cost= 2.832105774422263\n",
      "Epoch 9, cost= 2.3586498567272884\n",
      "Epoch 10, cost= 1.9433652651513165\n",
      "Epoch 11, cost= 1.6225692376203025\n",
      "Epoch 12, cost= 1.3615707165755164\n",
      "Epoch 13, cost= 1.1293913494581778\n",
      "Epoch 14, cost= 0.9589313662846374\n",
      "Epoch 15, cost= 0.7887930416909036\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9221000075340271\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "  \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Introduction\n",
    "Tensorboard is a visualization tool used in tensorflow. It help developer easily illustrate useful information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1, cost= 79.33581046537928\n",
      "Epoch 2, cost= 15.671157075275076\n",
      "Epoch 3, cost= 9.610744973529467\n",
      "Epoch 4, cost= 6.865333985740492\n",
      "Epoch 5, cost= 5.197681124968961\n",
      "Epoch 6, cost= 4.114182637929916\n",
      "Epoch 7, cost= 3.309836923283609\n",
      "Epoch 8, cost= 2.711409770263864\n",
      "Epoch 9, cost= 2.238252088237892\n",
      "Epoch 10, cost= 1.8440492352030478\n",
      "Epoch 11, cost= 1.5413093381849852\n",
      "Epoch 12, cost= 1.280805748368518\n",
      "Epoch 13, cost= 1.0719211351157119\n",
      "Epoch 14, cost= 0.8993942561862064\n",
      "Epoch 15, cost= 0.7580923894155875\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9203000068664551\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.reset_default_graph()\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "  \n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    out_1 = tf.nn.relu(layer_1)\n",
    "    tf.summary.histogram(\"relu1\", out_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(out_1, weights['h2']), biases['b2'])\n",
    "    out_2 = tf.nn.relu(layer_2)\n",
    "    tf.summary.histogram(\"relu2\", out_2)\n",
    "    \n",
    "    out_layer = tf.matmul(out_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "with tf.name_scope('DNN_Model'):\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "with tf.name_scope('Cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "with tf.name_scope('SGD'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "# Create summaries to visualize weights\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name.replace(':','_'), var)\n",
    "\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter('./tensorboard_data', graph=tf.get_default_graph())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            # Write logs at every iteration\n",
    "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch {}, cost= {}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Accuracy: {}\".format(accuracy.eval({x: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `tensorboard --logdir [directory_name]` to monitor everything you like.\n",
    "\n",
    "\n",
    "![Alt text](./images/dnn_implement/scalars.png)\n",
    "![Alt text](./images/dnn_implement/tensor_graph.png)\n",
    "![Alt text](./images/dnn_implement/histograms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
