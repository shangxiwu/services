{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.12999999523162842\n",
      "step 100, training accuracy 0.6399999856948853\n",
      "step 200, training accuracy 0.8899999856948853\n",
      "step 300, training accuracy 0.8999999761581421\n",
      "step 400, training accuracy 0.8999999761581421\n",
      "step 500, training accuracy 0.9399999976158142\n",
      "step 600, training accuracy 0.9599999785423279\n",
      "step 700, training accuracy 0.9599999785423279\n",
      "step 800, training accuracy 0.9300000071525574\n",
      "step 900, training accuracy 0.9399999976158142\n",
      "test accuracy 0.9697999954223633\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial_value = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial_value)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial_value = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial_value)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # strides = [1, horizontal strides, vertical strides, 1], padding = SAME mean add paddings on input data\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # ksize = [1, width, height, 1]\n",
    "    # strides = [1, horizontal strides, vertical strides, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Reshape our data as 4-D tensorflow. Remind that a batch of images is a 4-D tensor\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# Flatten previous layer result and feed them into fully connected layer\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "# Define cost\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# Optimization \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Calculate accuracy \n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        \n",
    "        train_step_ = sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "            print('step {}, training accuracy {}'.format(i, train_accuracy))\n",
    "            \n",
    "    test_accuracy_ = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})    \n",
    "    print('test accuracy {}'.format(test_accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notMNIST\n",
    "\n",
    "notMNIST dataset consist of 10 classes. Each image is a letter(from A-J). Following are examples of letter \"A\"\n",
    "\n",
    "![Alt text](./images/cnn_implement/notmnist.png)\n",
    "\n",
    "more reference http://yaroslavvb.blogspot.tw/2011/09/notmnist-dataset.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download dataset. Following is download script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "After reformat......\n",
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f, encoding='latin1')\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "'''\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "    convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "    labels as float 1-hot encodings.\n",
    "'''\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('After reformat......')\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0\n",
      "Minibatch accuracy: 0.125\n",
      "Minibatch loss at step 100\n",
      "Minibatch accuracy: 0.375\n",
      "Minibatch loss at step 200\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 300\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 400\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 500\n",
      "Minibatch accuracy: 0.875\n",
      "Minibatch loss at step 600\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 700\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 800\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 900\n",
      "Minibatch accuracy: 0.875\n",
      "Minibatch loss at step 1000\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 1100\n",
      "Minibatch accuracy: 0.75\n",
      "Minibatch loss at step 1200\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 1300\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 1400\n",
      "Minibatch accuracy: 0.625\n",
      "Minibatch loss at step 1500\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 1600\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 1700\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 1800\n",
      "Minibatch accuracy: 0.875\n",
      "Minibatch loss at step 1900\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 2000\n",
      "Minibatch accuracy: 0.8125\n",
      "Testing......\n",
      "Test accuracy: 0.9128000140190125\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, num_labels])\n",
    "\n",
    "# Parameters\n",
    "layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "# Define Model\n",
    "def model(input_image):\n",
    "    conv1 = tf.nn.conv2d(input_image, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    \n",
    "    # Flatten previous layer result\n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # Feed into fully connected layer\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "# Training computation.\n",
    "logits = model(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "num_steps = 2001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = { x: batch_data, y: batch_labels}\n",
    "        _, l, train_accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step {}'.format(step, l))\n",
    "            print('Minibatch accuracy: {}'.format(train_accuracy_))\n",
    "    \n",
    "    '''\n",
    "    Testing Part\n",
    "    '''        \n",
    "    print('Testing......')\n",
    "    feed_dict = { x: test_dataset, y: test_labels}\n",
    "    test_accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('Test accuracy: {}'.format(test_accuracy_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add regularization on the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "After reformat......\n",
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0\n",
      "Minibatch accuracy: 0.1875\n",
      "Minibatch loss at step 100\n",
      "Minibatch accuracy: 0.625\n",
      "Minibatch loss at step 200\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 300\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 400\n",
      "Minibatch accuracy: 1.0\n",
      "Minibatch loss at step 500\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 600\n",
      "Minibatch accuracy: 0.75\n",
      "Minibatch loss at step 700\n",
      "Minibatch accuracy: 0.75\n",
      "Minibatch loss at step 800\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 900\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 1000\n",
      "Minibatch accuracy: 0.8125\n",
      "Minibatch loss at step 1100\n",
      "Minibatch accuracy: 0.75\n",
      "Minibatch loss at step 1200\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 1300\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 1400\n",
      "Minibatch accuracy: 0.625\n",
      "Minibatch loss at step 1500\n",
      "Minibatch accuracy: 0.875\n",
      "Minibatch loss at step 1600\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 1700\n",
      "Minibatch accuracy: 0.6875\n",
      "Minibatch loss at step 1800\n",
      "Minibatch accuracy: 0.75\n",
      "Minibatch loss at step 1900\n",
      "Minibatch accuracy: 0.9375\n",
      "Minibatch loss at step 2000\n",
      "Minibatch accuracy: 0.875\n",
      "Testing......\n",
      "Test accuracy: 0.9142000079154968\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f, encoding='latin1')\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "    \n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "'''\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "    convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "    labels as float 1-hot encodings.\n",
    "'''\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('After reformat......')\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "beta = 0.01\n",
    "x = tf.placeholder(tf.float32, [None, image_size, image_size, num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, num_labels])\n",
    "\n",
    "# Parameters\n",
    "layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "# Define Model\n",
    "def model(input_image):\n",
    "    conv1 = tf.nn.conv2d(input_image, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    conv2 = tf.nn.conv2d(hidden1, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "# Training computation.\n",
    "logits = model(x)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\\\n",
    "                      +beta*tf.nn.l2_loss(layer1_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer2_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer3_weights)\n",
    "                      +beta*tf.nn.l2_loss(layer4_weights)\n",
    "                     \n",
    "                     )\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "num_steps = 2001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = { x: batch_data, y: batch_labels}\n",
    "        _, l, train_accuracy_ = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step {}'.format(step, l))\n",
    "            print('Minibatch accuracy: {}'.format(train_accuracy_))\n",
    "    \n",
    "    '''\n",
    "    Testing Part\n",
    "    '''        \n",
    "    print('Testing......')\n",
    "    feed_dict = { x: test_dataset, y: test_labels}\n",
    "    test_accuracy_ = sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('Test accuracy: {}'.format(test_accuracy_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cifar10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 classification is a common benchmark problem in machine learning. It consists of 60000 32x32 colour images in 10 classes and 6000 images per class\n",
    "\n",
    "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck\n",
    "\n",
    "![Alt text](./images/cnn_implement/cifar10.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use DNN to solve cifar10 and then compare with CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, cost=70371.31165865387\n",
      "Epoch: 101, cost=421.26674522986764\n",
      "Epoch: 201, cost=192.54563093919023\n",
      "Epoch: 301, cost=115.88981913542136\n",
      "Epoch: 401, cost=78.60732570550383\n",
      "Opitimization Finished!\n",
      "Accuracy: 0.4807\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        d = pickle.load(fo, encoding='latin1')\n",
    "    return d\n",
    "    \n",
    "\n",
    "def onehot(labels):\n",
    "    ''' one-hot encoding'''\n",
    "    n_sample = len(labels)\n",
    "    n_class = max(labels) + 1\n",
    "    onehot_labels = np.zeros((n_sample, n_class))\n",
    "    onehot_labels[np.arange(n_sample), labels] = 1\n",
    "\n",
    "    return onehot_labels\n",
    "\n",
    "\n",
    "data1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
    "data2 = unpickle('cifar-10-batches-py/data_batch_2')\n",
    "data3 = unpickle('cifar-10-batches-py/data_batch_3')\n",
    "data4 = unpickle('cifar-10-batches-py/data_batch_4')\n",
    "data5 = unpickle('cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "X_train = np.concatenate((data1['data'], data2['data'], data3['data'], data4['data'], data5['data']), axis=0) / 255.0\n",
    "label = np.concatenate((data1['labels'], data2['labels'], data3['labels'], data4['labels'], data5['labels']), axis=0)\n",
    "y_train = onehot(label)\n",
    "\n",
    "test = unpickle('cifar-10-batches-py/test_batch')\n",
    "X_test = test['data'] / 255.0\n",
    "y_test = onehot(test['labels'])\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 500\n",
    "batch_size = 256\n",
    "display_step = 100\n",
    "n_sample = X_train.shape[0]\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden_1 = 1024\n",
    "n_hidden_2 = 1024\n",
    "n_hidden_3 = 1024\n",
    "n_class = y_train.shape[1]\n",
    "\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_class])\n",
    "\n",
    "\n",
    "def multiplayer_perceptron(x, weight, bias):\n",
    "\n",
    "    layer1 = tf.add(tf.matmul(x, weight['h1']), bias['h1'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    layer2 = tf.add(tf.matmul(layer1, weight['h2']), bias['h2'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    layer3 = tf.add(tf.matmul(layer2, weight['h3']), bias['h3'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    out_layer = tf.add(tf.matmul(layer3, weight['out']), bias['out'])\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "weight = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), \n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])), \n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_class]))\n",
    "}\n",
    "bias = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_2])), \n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_3])), \n",
    "    'out': tf.Variable(tf.random_normal([n_class]))\n",
    "}\n",
    "\n",
    "pred = multiplayer_perceptron(x, weight, bias)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(n_sample / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: X_train[i*batch_size : (i+1)*batch_size, :], \n",
    "                                                          y: y_train[i*batch_size : (i+1)*batch_size, :]})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        if epoch % display_step == 0:\n",
    "            print('Epoch: {}, cost={}'.format(epoch+1, avg_cost))\n",
    "\n",
    "            \n",
    "    print('Opitimization Finished!')\n",
    "    acc = sess.run(accuracy, feed_dict={x: X_test, y: y_test})\n",
    "    \n",
    "    print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can try to download CNN on cifar10 from the following link:  \n",
    "https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/\n",
    "\n",
    "After downloading, use ``python cifar10_train.py`` to train your network and ``python cifar10_eval.py`` to test your accuracy. For more information please reference here  \n",
    "https://www.tensorflow.org/tutorials/deep_cnn\n",
    "\n",
    "It needs more computation power but achieve about 86% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
