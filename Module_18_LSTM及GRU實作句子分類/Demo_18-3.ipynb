{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LSTM做MNIST分類 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-d809c92a7b4a>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-d809c92a7b4a>:38: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-d809c92a7b4a>:52: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-1-d809c92a7b4a>:62: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Step 1, Minibatch Loss= 2.4611, Training Accuracy= 0.039\n",
      "Step 200, Minibatch Loss= 2.1297, Training Accuracy= 0.266\n",
      "Step 400, Minibatch Loss= 1.9548, Training Accuracy= 0.375\n",
      "Step 600, Minibatch Loss= 1.7404, Training Accuracy= 0.484\n",
      "Step 800, Minibatch Loss= 1.6666, Training Accuracy= 0.438\n",
      "Step 1000, Minibatch Loss= 1.6506, Training Accuracy= 0.469\n",
      "Step 1200, Minibatch Loss= 1.6211, Training Accuracy= 0.445\n",
      "Step 1400, Minibatch Loss= 1.4110, Training Accuracy= 0.586\n",
      "Step 1600, Minibatch Loss= 1.3546, Training Accuracy= 0.586\n",
      "Step 1800, Minibatch Loss= 1.2585, Training Accuracy= 0.594\n",
      "Step 2000, Minibatch Loss= 1.1430, Training Accuracy= 0.641\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.59375\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 2000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "\n",
    "    # Define a cell with tensorflow\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "    #cell = tf.contrib.rnn.GRUCell(num_hidden)\n",
    "    \n",
    "    ## switch to multiple layer RNN\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(num_hidden) for _ in range(3)])\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    # Get cell output\n",
    "    \n",
    "    #if inputs is (batches, steps, inputs) ==> time_major=False\n",
    "    #if inputs is (steps, batches, inputs) ==> time_major=True\n",
    "    #state_is_tuple = Treu mean output of dynamic_rnn is (c_state, h_state)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state, time_major=False)\n",
    "    # change outputs to list [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits , 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GRU做MNIST分類 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-2-dd714447491c>:38: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Step 1, Minibatch Loss= 3.3061, Training Accuracy= 0.086\n",
      "Step 200, Minibatch Loss= 1.9992, Training Accuracy= 0.297\n",
      "Step 400, Minibatch Loss= 1.7737, Training Accuracy= 0.445\n",
      "Step 600, Minibatch Loss= 1.6202, Training Accuracy= 0.500\n",
      "Step 800, Minibatch Loss= 1.3696, Training Accuracy= 0.562\n",
      "Step 1000, Minibatch Loss= 1.4212, Training Accuracy= 0.531\n",
      "Step 1200, Minibatch Loss= 1.3958, Training Accuracy= 0.578\n",
      "Step 1400, Minibatch Loss= 1.1906, Training Accuracy= 0.617\n",
      "Step 1600, Minibatch Loss= 1.3585, Training Accuracy= 0.594\n",
      "Step 1800, Minibatch Loss= 1.1353, Training Accuracy= 0.711\n",
      "Step 2000, Minibatch Loss= 1.0631, Training Accuracy= 0.680\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.59375\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 2000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "\n",
    "    # Define a cell with tensorflow\n",
    "    cell = tf.contrib.rnn.GRUCell(num_hidden)\n",
    "    \n",
    "    ## switch to multiple layer RNN\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(num_hidden) for _ in range(3)])\n",
    "    \n",
    "    init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    # Get cell output\n",
    "    \n",
    "    #if inputs is (batches, steps, inputs) ==> time_major=False\n",
    "    #if inputs is (steps, batches, inputs) ==> time_major=True\n",
    "    #state_is_tuple = Treu mean output of dynamic_rnn is (c_state, h_state)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state=init_state, time_major=False)\n",
    "    # change outputs to list [(batch, outputs)..] * steps\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1,0,2]))\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits , 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LSTM做垃圾郵件分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0 \n",
      "text_data_target: ham \n",
      "text_data_train: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "i = 1 \n",
      "text_data_target: ham \n",
      "text_data_train: Ok lar... Joking wif u oni...\n",
      "i = 2 \n",
      "text_data_target: spam \n",
      "text_data_train: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "i = 3 \n",
      "text_data_target: ham \n",
      "text_data_train: U dun say so early hor... U c already then say...\n",
      "i = 4 \n",
      "text_data_target: ham \n",
      "text_data_train: Nah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Set RNN parameters\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "max_sequence_length = 25\n",
    "rnn_size = 10\n",
    "embedding_size = 50\n",
    "min_word_frequency = 10\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Download or open data\n",
    "data_dir = 'temp'\n",
    "data_file = 'text_data.txt'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "\n",
    "    # Save data to text file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as file_conn:\n",
    "        for text in text_data:\n",
    "            file_conn.write(\"{}\\n\".format(text))\n",
    "else:\n",
    "    # Open data from text file\n",
    "    text_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        for row in file_conn:\n",
    "            text_data.append(row)\n",
    "    text_data = text_data[:-1]\n",
    "\n",
    "text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "\n",
    "for i in range(5):\n",
    "    print('i = {} \\ntext_data_target: {} \\ntext_data_train: {}'.format(i, text_data_target[i], text_data_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat\n",
      "ok lar joking wif u oni\n",
      "free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry questionstd txt ratetcs apply overs\n",
      "u dun say so early hor u c already then say\n",
      "nah i dont think he goes to usf he lives around here though\n",
      "freemsg hey there darling its been weeks now and no word back id like some fun you up for it still tb ok xxx std chgs to send to rcv\n",
      "even my brother is not like to speak with me they treat me like aids patent\n",
      "as per your request melle melle oru minnaminunginte nurungu vettam has been set as your callertune for all callers press to copy your friends callertune\n",
      "winner as a valued network customer you have been selected to receivea prize reward to claim call claim code kl valid hours only\n",
      "had your mobile months or more u r entitled to update to the latest colour mobiles with camera for free call the mobile update co free on\n",
      "WARNING:tensorflow:From <ipython-input-4-dbaefe3edd4b>:16: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "=================\n",
      "[ 44 455   0 809 703 667  62   9   0  87 120 366   0 152   0   0  66  56\n",
      "   0 136   0   0   0   0   0]\n",
      "[ 47 315   0 458   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[ 46 465   9   4 773 882   1 178   0   0 622   0 236 258  69   0   1   1\n",
      " 318 465   0  77   0 368   0]\n",
      "[  6 230 140  23 357   0   6 155 142  58 140   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[  0   2  42  97  70 466   1 928  70   0 202 110 473   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return(text_string)\n",
    "\n",
    "# Clean texts\n",
    "text_data_train = [clean_text(x) for x in text_data_train]\n",
    "\n",
    "# print first 10 text data\n",
    "for i in range(10):\n",
    "    print(text_data_train[i])\n",
    "\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n",
    "                                                                     min_frequency=min_word_frequency)\n",
    "text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))\n",
    "\n",
    "print('=================')\n",
    "\n",
    "for i in range(5):\n",
    "    print(text_processed[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 933\n",
      "80-20 Train Test split: 4459 -- 1115\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split data\n",
    "text_processed = np.array(text_processed)\n",
    "text_data_target = np.array([1 if x=='ham' else 0 for x in text_data_target])\n",
    "shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\n",
    "x_shuffled = text_processed[shuffled_ix]\n",
    "y_shuffled = text_data_target[shuffled_ix]\n",
    "\n",
    "# Split train/test set\n",
    "ix_cutoff = int(len(y_shuffled)*0.80)\n",
    "x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "print(\"80-20 Train Test split: {:d} -- {:d}\".format(len(y_train), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-a85aa792689b>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-a85aa792689b>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\isaac\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Create placeholders\n",
    "x_data = tf.placeholder(tf.int32, [None, max_sequence_length])\n",
    "y_output = tf.placeholder(tf.int32, [None])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Create embedding\n",
    "embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)\n",
    "\n",
    "cell=tf.contrib.rnn.BasicRNNCell(num_units = rnn_size)\n",
    "#cell = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True)\n",
    "#cell = tf.contrib.rnn.GRUCell(rnn_size)\n",
    "\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
    "output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "\n",
    "# Get output of RNN sequence\n",
    "\n",
    "output = tf.unstack(tf.transpose(output, [1,0,2]))\n",
    "last = output[-1]\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "logits_out = tf.matmul(last, weight) + bias\n",
    "\n",
    "# Loss function\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output) # logits=float32, labels=int32\n",
    "loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test Loss: 0.41, Test Acc: 0.86\n",
      "Epoch: 2, Test Loss: 0.37, Test Acc: 0.87\n",
      "Epoch: 3, Test Loss: 0.36, Test Acc: 0.88\n",
      "Epoch: 4, Test Loss: 0.33, Test Acc: 0.88\n",
      "Epoch: 5, Test Loss: 0.28, Test Acc: 0.89\n",
      "Epoch: 6, Test Loss: 0.24, Test Acc: 0.92\n",
      "Epoch: 7, Test Loss: 0.21, Test Acc: 0.94\n",
      "Epoch: 8, Test Loss: 0.18, Test Acc: 0.95\n",
      "Epoch: 9, Test Loss: 0.16, Test Acc: 0.95\n",
      "Epoch: 10, Test Loss: 0.15, Test Acc: 0.95\n",
      "Epoch: 11, Test Loss: 0.14, Test Acc: 0.96\n",
      "Epoch: 12, Test Loss: 0.13, Test Acc: 0.96\n",
      "Epoch: 13, Test Loss: 0.13, Test Acc: 0.96\n",
      "Epoch: 14, Test Loss: 0.14, Test Acc: 0.96\n",
      "Epoch: 15, Test Loss: 0.15, Test Acc: 0.95\n",
      "Epoch: 16, Test Loss: 0.11, Test Acc: 0.97\n",
      "Epoch: 17, Test Loss: 0.1, Test Acc: 0.97\n",
      "Epoch: 18, Test Loss: 0.098, Test Acc: 0.97\n",
      "Epoch: 19, Test Loss: 0.11, Test Acc: 0.97\n",
      "Epoch: 20, Test Loss: 0.091, Test Acc: 0.97\n",
      "Epoch: 21, Test Loss: 0.11, Test Acc: 0.98\n",
      "Epoch: 22, Test Loss: 0.084, Test Acc: 0.97\n",
      "Epoch: 23, Test Loss: 0.093, Test Acc: 0.97\n",
      "Epoch: 24, Test Loss: 0.089, Test Acc: 0.97\n",
      "Epoch: 25, Test Loss: 0.087, Test Acc: 0.97\n",
      "Epoch: 26, Test Loss: 0.12, Test Acc: 0.97\n",
      "Epoch: 27, Test Loss: 0.081, Test Acc: 0.98\n",
      "Epoch: 28, Test Loss: 0.098, Test Acc: 0.97\n",
      "Epoch: 29, Test Loss: 0.081, Test Acc: 0.98\n",
      "Epoch: 30, Test Loss: 0.092, Test Acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    # Start training\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Shuffle training data\n",
    "        shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "        x_train = x_train[shuffled_ix]\n",
    "        y_train = y_train[shuffled_ix]\n",
    "        num_batches = int(len(x_train)/batch_size) + 1\n",
    "        # TO DO CALCULATE GENERATIONS ExACTLY\n",
    "        for i in range(num_batches):\n",
    "            # Select train data\n",
    "            min_ix = i * batch_size\n",
    "            max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n",
    "            x_train_batch = x_train[min_ix:max_ix]\n",
    "            y_train_batch = y_train[min_ix:max_ix]\n",
    "\n",
    "            # Run train step\n",
    "            train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}\n",
    "            sess.run(train_step, feed_dict=train_dict)\n",
    "\n",
    "        # Run loss and accuracy for training\n",
    "        temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_accuracy.append(temp_train_acc)\n",
    "\n",
    "        # Run Eval Step\n",
    "        test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}\n",
    "        temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n",
    "        test_loss.append(temp_test_loss)\n",
    "        test_accuracy.append(temp_test_acc)\n",
    "        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
